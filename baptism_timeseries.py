# -*- coding: utf-8 -*-
"""Bridges over time Baptism Timeseries Network

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-xUNQxVywRwCUHtU-cOZMOQydvzPlKmr

[Sample csv](https://github.com/mkane2/baptisms/blob/main/DRC%20Schoharie/drc-schoharie.csv)
"""

# !pip install gender_guesser

# !pip install --upgrade scipy networkx

# !pip install python-louvain
import community.community_louvain as community_louvain

import csv
# import urllib.request
import json
# from operator import itemgetter 
from itertools import combinations

from networkx.readwrite import json_graph 
import networkx as nx 
# from networkx.algorithms import community 
from networkx.algorithms.approximation import average_clustering

import matplotlib.pyplot as plt
import pandas as pd
# from datetime import date, timedelta, datetime
# from dateutil.relativedelta import relativedelta
import numpy as np

import gender_guesser.detector as gender
import os
# from os.path import exists

# from google.colab import files
import shutil

remote_files = [
   # {
   #   "congregation": "drc-albany",
   #   "filename": "https://raw.githubusercontent.com/mkane2/baptisms/main/edge_lists/drc-albany.csv",
   #   "denomination": "dutch reformed",
   #   "latitude": 42.6536,
   #   "longitude": -73.7506
   # },
  # {
  #   "congregation": "ny-kingston-drc",
  #   "filename": "https://raw.githubusercontent.com/mkane2/baptisms/main/edge_lists/ny-kingston-drc.csv",
  #   "denomination": "dutch reformed",
  #   "latitude": 41.927,
  #   "longitude": -73.9974
  # },
   {
     "congregation": "ny-nyc-drc",
     "filename": "https://raw.githubusercontent.com/mkane2/baptisms/main/edge_lists/ny-nyc-drc.csv",
     "denomination": "dutch reformed",
     "latitude": 40.745556,
     "longitude": -73.9875
   },
   # {
   #   "congregation": "pa-aaronsberg-lutheran",
   #   "filename": "https://raw.githubusercontent.com/mkane2/baptisms/main/edge_lists/pa-aaronsberg-lutheran.csv",
   #   "denomination": "lutheran",
   #   "latitude": 40.89869,
   #   "longitude": -77.456184
   # },
  # {
  #   "congregation": "pa-canadochly-lutheran",
  #   "filename": "https://raw.githubusercontent.com/mkane2/baptisms/main/edge_lists/pa-canadochly-lutheran.csv",
  #   "denomination": "lutheran",
  #   "latitude": 40.902496,
  #   "longitude": -77.833451
  # },
  # {
  #   "congregation": "pa-derry-lutheran",
  #   "filename": "https://raw.githubusercontent.com/mkane2/baptisms/main/edge_lists/pa-derry-lutheran.csv",
  #   "denomination": "lutheran",
  #   "latitude": 40.334,
  #   "longitude": -79.2998
  # },
  # {
  #   "congregation": "pa-egypt-grc",
  #   "filename": "https://raw.githubusercontent.com/mkane2/baptisms/main/edge_lists/pa-egypt-grc.csv",
  #   "denomination": "german reformed",
  #   "latitude": 40.680096,
  #   "longitude": -75.529907
  # },
  # {
  #   "congregation": "pa-himmel-lutheran",
  #   "filename": "https://raw.githubusercontent.com/mkane2/baptisms/main/edge_lists/pa-himmel-lutheran.csv",
  #   "denomination": "lutheran",
  #   "latitude": 40.7231,
  #   "longitude": -76.737
  # },
  # {
  #   "congregation": "pa-howerters-grc",
  #   "filename": "https://raw.githubusercontent.com/mkane2/baptisms/main/edge_lists/pa-howerters-grc.csv",
  #   "denomination": "german reformed",
  #   "latitude": 39.78969,
  #   "longitude": -76.977984
  # },
  # {
  #   "congregation": "pa-macungie-lutheran",
  #   "filename": "https://raw.githubusercontent.com/mkane2/baptisms/main/edge_lists/pa-macungie-lutheran.csv",
  #   "denomination": "lutheran",
  #   "latitude": 40.517851,
  #   "longitude": -75.565028
  # },
  # {
  #   "congregation": "pa-philadelphia-grc",
  #   "filename": "https://raw.githubusercontent.com/mkane2/baptisms/main/edge_lists/pa-philadelphia-grc.csv",
  #   "denomination": "german reformed",
  #   "latitude": 39.9526,
  #   "longitude": -75.1652
  # },
  # {
  #   "congregation": "pa-schuykill-dunkels-lutheran",
  #   "filename": "https://raw.githubusercontent.com/mkane2/baptisms/main/edge_lists/pa-schuykill-dunkels-lutheran.csv",
  #   "denomination": "lutheran",
  #   "latitude": 40.902496,
  #   "longitude": -77.833451
  # },
  # {
  #   "congregation": "va-peakedmountain-union",
  #   "filename": "https://raw.githubusercontent.com/mkane2/baptisms/main/edge_lists/va-peakedmountain-union.csv",
  #   "denomination": "union",
  #   "latitude": 36.577931,
  #   "longitude": -76.035763
  # },
    # {
  #   "congregation": "nj-oldwick-lutheran",
  #   "filename": "https://raw.githubusercontent.com/mkane2/baptisms/main/edge_lists/nj-oldwick-lutheran.csv",
  #   "denomination": "lutheran",
  #   "latitude": 40.6726,
  #   "longitude": -74.74738
  # },
    # {
  #   "congregation": "pa-whitehall-union",
  #   "filename": "https://raw.githubusercontent.com/mkane2/baptisms/main/edge_lists/pa-whitehall-union.csv",
  #   "denomination": "union",
  #   "latitude": 40.656746,
  #   "longitude": -75.504128
  # },
]
statsdfs = []
degdfs = []

female_names = pd.read_csv("https://raw.githubusercontent.com/mkane2/baptisms/main/names/female_names.csv")
male_names = pd.read_csv("https://raw.githubusercontent.com/mkane2/baptisms/main/names/male_names.csv")
gc = gender.Detector()

def setup(c, f, d, lt, ln):
  global congregation
  global denomination
  global latitude
  global longitude
  global filename
  global df
  global start_date
  global end_date
  congregation = c
  filename = f
  denomination = d
  latitude = lt
  longitude = ln
  df = pd.read_csv(filename)
  #FINDING MAX AND MIN
  start_date = df['year'].min()
  end_date = df['year'].max()
  directory = './' + congregation + '/'
  if os.path.exists(directory):
      for filename in os.listdir(directory):
        file_path = os.path.join(directory, filename)
        try:
            if os.path.isfile(file_path) or os.path.islink(file_path):
                os.unlink(file_path)
        except Exception as e:
            print('Failed to delete %s. Reason: %s' % (file_path, e))
  if not os.path.exists(directory):
      os.mkdir(directory)

def getedges(): 
  global edges
  # response = urllib.request.urlopen(filename)
  # lines = [l.decode('utf-8') for l in response.readlines()]
  with open("edge_lists/" + congregation + ".csv") as csv_file:
      cr = csv.reader(csv_file)
      edges = [tuple(e) for e in cr][1:]
      # for e in edges:
      #     print(e)

"""## Make the initial graph"""

def makegraph():
  global G
  G = nx.Graph()
  for e in edges:
    calc_end = int(e[0]) + 20
    if calc_end > end_date:
      calc_end = end_date
    G.add_edge(e[1], e[2], start=int(e[0]), end=calc_end)

  G = nx.relabel.convert_node_labels_to_integers(G, first_label=0, ordering='default', label_attribute="name")
  G.remove_edges_from(nx.selfloop_edges(G))

"""## Extract the giant component"""

def make_giant():  
  global L
  global connected_subgraphs
  # find the number of subgraphs in the overall network
  connected_subgraphs = [G.subgraph(cc) for cc in nx.connected_components(G)]
  print("Number of subgraphs: " + str(len(connected_subgraphs)))

  # get the giant component of the overall network
  Gcc = sorted(nx.connected_components(G), key=len, reverse=True)
  L = G.subgraph(Gcc[0])
  print("total network", G)
  print("largest subgraph", L)
  if len(connected_subgraphs) > 1:
    K = G.subgraph(Gcc[1])
    print("second largest subgraph", K)

  global giant_percent_edges
  global giant_percent_nodes
  global k_sample
  k_sample = L.number_of_nodes()
  giant_percent_nodes = L.number_of_nodes()/G.number_of_nodes()
  giant_percent_edges = L.number_of_edges()/G.number_of_edges()
  # print(giant_percent_nodes, giant_percent_edges)

# L.nodes[0]

"""## Get degree sequence"""

def get_degseq():
  global degrees
  degrees = [val for (node, val) in sorted(L.degree(), key=lambda x: x[-1])]
# print(degrees)

"""## Get triads"""

def get_triads():
  global triad_class
  triad_class = {}
  for nodes in combinations(L.nodes, 3):
      n_edges = L.subgraph(nodes).number_of_edges()
      triad_class.setdefault(n_edges, []).append(nodes)

# print("unconnected", len(triad_class[0]))
# print("dyad", len(triad_class[1]))
# print("incomplete triad", len(triad_class[2]))
# print("complete triad", len(triad_class[3]))

"""## Extract nodes"""

def get_nodes():
  global nodedf
  rows = list(G.nodes(data=True))
  listofdicts = []
  for n in rows:
    thisdict = {"id": n[0], "name": n[1]['name']}
    listofdicts.append(thisdict)
  nodedf = pd.DataFrame(listofdicts) 
  nodedf.to_csv(congregation + '/' + congregation + '_nodelist.csv', index=False)

"""## Assign gender"""

def assign_gender():
  global nodegender
  data = congregation + '/' + congregation + '_nodelist.csv'
  gender_out = congregation + '/' + congregation + '_nodelist_gender.csv'

  record = 1
  with open(data, 'r') as file:
    for line in file:
      if record == 1:
        newline = line.strip().split(',')
        stuff = ['gender']
        newline.extend(stuff)
        with open(gender_out, 'a') as outputfile:
          outputwriter = csv.writer(outputfile)
          outputwriter.writerow(newline)
      else:
        # print(line)
        name = line.split(',')[1]
        first = name.split(' ')[0]
        get_this = gc.get_gender(first)
        # gender = get_this['gender']
        # prob = get_this['probability']
        newline = line.strip().split(',')
        newline.extend([get_this])
        with open(gender_out, 'a') as outputfile:
            outputwriter = csv.writer(outputfile)
            outputwriter.writerow(newline)
      record += 1

  nodegender = pd.read_csv(gender_out)
  nodegender['name'] = nodegender['name'].astype("string")
  nodegender['gender'].fillna('unknown', inplace=True)
  nodegender.loc[nodegender['gender'] == 'andy', 'gender'] = "unknown"
  nodegender.loc[nodegender['gender'] == 'mostly_female', 'gender'] = "female"
  nodegender.loc[nodegender['gender'] == 'mostly_male', 'gender'] = "male"

# nodegender.gender.value_counts()

# nodegender.loc[nodegender['gender'] == "unknown"]

def gender_unknown():
  print(nodegender.gender.value_counts())
  nodegender.dropna(inplace=True)
  for index, row in nodegender.loc[nodegender['gender'] == "unknown"].iterrows():
    first = row['name'].split(' ')[0]
    # print(first)
    if (first in female_names['name'].values):
      nodegender.loc[nodegender['name'] == row['name'], 'gender'] = "female"
      # print(first, "female")
    if (first in male_names['name'].values):
      nodegender.loc[nodegender['name'] == row['name'], 'gender'] = "male"
      # print(first, "male")
  print(nodegender.gender.value_counts())

# nodegender.loc[nodegender['gender'] == "unknown"]

def set_gender():
  # nodegender.reset_index(drop=True, inplace=True)
  node_attr = nodegender.set_index('id').to_dict('index')
  nx.set_node_attributes(L, node_attr)
  # nodesUnknown = [x for x,y in L.nodes(data=True) if y['gender']=='unknown']
  # print(nodesUnknown)

# L.nodes[0]

def gender_assort(graph):
  global gender_assortivity
  gender_assortivity = nx.attribute_assortativity_coefficient(graph, attribute='gender')
  # print(gender_assortivity)

"""## Assign race"""

def assign_race():
  print("assign race")
  nodegender['race'] = ""
  nodegender.loc[nodegender['name'].str.contains("Indian"), 'race'] = "Native"
  nodegender.loc[nodegender['name'].str.contains("Negro"), 'race'] = "Black"
  nodegender.loc[nodegender['name'].str.contains("negro"), 'race'] = "Black"
  nodegender.loc[nodegender['name'].str.contains("Slave"), 'race'] = "Black"
  nodegender.loc[nodegender['name'].str.contains("slave"), 'race'] = "Black"
  nodegender.loc[nodegender['name'].str.contains("servant"), 'race'] = "Black"
  nodegender.loc[nodegender['name'].str.contains("Servant"), 'race'] = "Black"
  nodegender.loc[nodegender['name'].str.contains("black"), 'race'] = "Black"
  nodegender.loc[nodegender['name'].str.contains("Black"), 'race'] = "Black"
  nodegender.loc[nodegender['race'] == '', 'race'] = "white"
  race_out = congregation + '/' + congregation + "_nodelist_gender_race.csv"
  nodegender.to_csv(race_out,index=False)
  # nodegender

# nodegender.race.value_counts()

def set_race():
  print("set race")
  noderace = nodegender.drop(columns=["gender"])
  raceattr = noderace.set_index('id').to_dict('index')
  # noderace
  nx.set_node_attributes(L, raceattr)

"""## Find bridges"""

def set_bridges(graph):
  print("set bridges")
  global male_bridges
  global female_bridges
  global bridges
  bridgeattr = "false"
  nx.set_node_attributes(graph, bridgeattr, "bridge")
  bridges = list(nx.articulation_points(graph))

  for b in bridges:
    if graph.nodes[b]['gender'] == 'female':
      nx.set_node_attributes(graph, {b:"female_bridge"}, 'bridge')
    else: 
      nx.set_node_attributes(graph, {b:"male_bridge"}, 'bridge')
  male_bridges = len([x for x,y in graph.nodes(data=True) if y['bridge']=="male_bridge"])
  female_bridges = len([x for x,y in graph.nodes(data=True) if y['bridge']=="female_bridge"])
  # print("male bridges", male_bridges, "female bridges", female_bridges)

"""## Build the list of dates"""

from copy import deepcopy

def changeInt(datadict):
  # deepcopy before data manipulation
  newdict = deepcopy(datadict)
  for key,value in datadict.items():
    newdict[key] = int(datadict[key])
  # for key,value in datadict.items():
  #     # recurse into nested dicts
  #     if isinstance(value, dict):
  #         newdict[key] = changeInt(datadict[key])
  #     # convert to string
  #     elif isinstance(value, datetime.datetime): 
  #         newdict[key] = int(datadict[key])

  return newdict

import time
def set_degree():
  print("set_degree")
  degree_dict = dict(L.degree(L.nodes()))
  # degree_dict = [dict([a, int(x)] for a, x in b.items()) for b in degree_dict] # new
  degree_dict = changeInt(degree_dict)
  print("set_degree degree_dict")
  nx.set_node_attributes(L, degree_dict, "degree")
  print("get betweenness")
  if k_sample >= 15000:
      k_sub = 2000
  else:
      k_sub = None
  print("k_sample", k_sample, "k_sub", k_sub)
  current_time = time.time()
  betweenness_dict = nx.betweenness_centrality(L, k=k_sub)
  print(time.time()-current_time)
  # betweenness_dict = nx.betweenness_centrality(L, k=k_sub)
  print("set_degree betweenness_dict")
  # betweenness_dict = [dict([a, int(x)] for a, x in b.items()) for b in betweenness_dict] # new
  betweenness_dict = changeInt(betweenness_dict) 

  nx.set_node_attributes(L, betweenness_dict, "betweenness")

def set_end():
  print("set_end")
  for n in L.nodes():
    # cnxedges = L.adj[n]
    thislist = []
    for nbr, datadict in G.adj[n].items():
      thislist.append(int(datadict['end']))
    sortedlist = sorted(thislist, reverse=True)
    nx.set_node_attributes(L, {n: int(sortedlist[0])}, 'end')

def find_node(gr, att, val):
    return any([node for node in gr.nodes(data=True) if node[1][att] == val])

def make_years():
  print("make_years")
  global listofrows
  global start_date
  # preferential_edges = []
  # jacc_edges = []
  listofrows = []
  M = None
  year_count = 1
  # while the start date is less than the end date
  while start_date <= (end_date + 1):
    print(start_date)
    # select only the edges that were created 20 years before the start date, up to the start date
    selected_edges = [(u,v) for u,v,e in L.edges(data=True) if int(e['start']) in range(start_date - 20, start_date + 1)]
    # print(selected_edges)
    # make a new graph with the time filtered edges
    H = nx.Graph(selected_edges)
    # print(H)
    # print("L", L.nodes.data())
    # print("H", H.nodes.data())
    # print("H edges", list(H.edges(data=True)))
    # print("H nodes", list(H.nodes(data=True)))
    for node in H.nodes():
      # print(L.nodes[node])
      attrs = L.nodes[node]
      H.nodes[node].update(attrs) 
      # nx.set_node_attributes(H, attrs)
      # print(H.nodes[node])
    if H.number_of_nodes() == 0:
      start_date += 1
      year_count += 1
      continue

    # make a dictionary for the filtered network's degree
    degree_dict = dict(H.degree(H.nodes()))
    # degree_dict = [dict([a, int(x)] for a, x in b.items()) for b in degree_dict] # new
    degree_dict = changeInt(degree_dict) 

    # write the start date and degree to the overall network
    f_date = start_date
    degree_time = 'degree_' + str(f_date)
    nx.set_node_attributes(L, 0, degree_time)
    nx.set_node_attributes(L, degree_dict, degree_time)
    
    # write the betweenness to the overall network
    if H.number_of_nodes() > 1000:
        k_sub = 500
    else:
        k_sub = None
    betweenness_dict = nx.betweenness_centrality(H, k=k_sub)
    # betweenness_dict = [dict([a, int(x)] for a, x in b.items()) for b in betweenness_dict] # new
    betweenness_dict = changeInt(betweenness_dict) 

    betweenness_time = 'between_' + str(f_date)
    nx.set_node_attributes(L, 0, betweenness_time)
    nx.set_node_attributes(L, betweenness_dict, betweenness_time)

    start_dict = dict()
    end_dict = dict()
    # M is the previous year graph
    # check if a node exists in the previous year M
    # if it does NOT exist in the previous year, write the f_date into the start_dict
    # if it does exist in the previous year and the current year, don't write anything
    # if it exists in the previous year but NOT the current year, write the f_date into the end_dict
    if M is None:
      for node in H.nodes():
        start_dict[node] = int(f_date)
    if M is not None:
      for node in H.nodes():
        node_dict = L.nodes[node]
        if M.has_node(node):
          pass
        elif node_dict.get('start') is None:
          start_dict[node] = int(f_date)
        else:
          pass
      
    nx.set_node_attributes(L, start_dict, "start")

    M = H

    # edge_predict = 0
    # for u, v, p in preferential_edges[-100:]:
    #   if (H.has_edge(u,v)):
    #     edge_predict += 1
    # jacc_predict = 0
    # for u, v, p in jacc_edges[-100:]:
    #   if (H.has_edge(u,v)):
    #     jacc_predict += 1
    
    # change over time
    density = nx.density(H)
    print("Network density", f_date, density)
    clustering = average_clustering(H)
    # preferential_predict = edge_predict/100
    # jacc_edges_predict = jacc_predict/100
    gender_assort = nx.attribute_assortativity_coefficient(H, attribute='gender')
    mod_assort = nx.attribute_assortativity_coefficient(H, attribute='modularity')
    degree_assort = nx.degree_assortativity_coefficient(H)
    print(gender_assort, mod_assort, degree_assort)
    
    # print("female bridges", female_bridges)
    bridgeattr = "false"
    nx.set_node_attributes(H, bridgeattr, "bridge")
    bridges = list(nx.articulation_points(H))
    
    for node in H.nodes:
        node_temp_dict = H.nodes[node]
        if 'gender' not in node_temp_dict or node_temp_dict['gender'] is None:
            print("This node has no gender", H.nodes[node].values())
            node_temp_dict['gender'] = 'unknown'
            
    for b in bridges:
      if H.nodes[b]['gender'] == 'female':
        nx.set_node_attributes(H, {b:"female_bridge"}, 'bridge')
      else: 
        nx.set_node_attributes(H, {b:"male_bridge"}, 'bridge')
    male_bridges = len([x for x,y in H.nodes(data=True) if y['bridge']=="male_bridge"])
    female_bridges = len([x for x,y in H.nodes(data=True) if y['bridge']=="female_bridge"])
    total_bridges = male_bridges + female_bridges
    if total_bridges > 0:
      bridge_pct = female_bridges/total_bridges
    else: 
      bridge_pct = 0
    
    mapping = {'male': 0, 'female': 1, 'unknown': 2}
    mixmat = nx.attribute_mixing_matrix(H, 'gender', mapping=mapping)
    mixmat[mapping['male'], mapping['female']]
    # count of attribute pairs
    # mix = nx.attribute_mixing_dict(H, "gender")

    if find_node(H, 'gender', 'male'):
      male_len = len([x for x,y in H.nodes(data=True) if y['gender']=="male"])
    else:
      male_len = 0
    if find_node(H, 'gender', 'female'):
      female_len = len([x for x,y in H.nodes(data=True) if y['gender']=="female"])
    else: 
      female_len = 0
    print(male_len, female_len)

    this_dict = {
        "year": f_date,
        "year_count": year_count,
        "congregation": congregation,
        "denomination": denomination,
        "density": density, 
        "clustering": clustering, 
        # "preferential_edges": preferential_predict, 
        # "jacc_edges": jacc_edges_predict,
        "gender_assort": gender_assort,
        "degree_assort": degree_assort,
        "mod_assort": mod_assort,
        "male_bridges": male_bridges,
        "female_bridges": female_bridges,
        "male-female": mixmat[mapping['male'], mapping['female']],
        "male-male": mixmat[mapping['male'], mapping['male']],
        "female-female": mixmat[mapping['female'], mapping['female']],
        "male": male_len,
        "female": female_len,
        "nodes": H.number_of_nodes(),
        "bridge_pct": bridge_pct,
        "network_pct": len([x for x,y in H.nodes(data=True) if y['gender']=="female"]) / H.number_of_nodes()
        }
    # print(this_dict)
    listofrows.append(this_dict)

    # predict nodes to be created
    # preds = nx.preferential_attachment(H)
    # preferential_edges = sorted(preds, key=lambda x: x[-1])
    # jacc = nx.jaccard_coefficient(H)
    # jacc_edges = sorted(jacc, key=lambda x: x[-1])
    
    nx.draw_networkx(H,node_size=20)
    plt.show()
    start_date += 1
    year_count += 1

"""### Export density over time"""

def export_density():
  print("export density")
  densitydf = pd.DataFrame(listofrows)
  densityname = congregation + '/' + congregation + '_density.csv'
  densitydf.to_csv(densityname)

"""## Find communities"""

def get_communities():
  print("get communities")
  global mod_assortivity
  global listcommunities
  # Detect the community structure of the graph which maximises the modularity using the Louvain heuristices
  partition = community_louvain.best_partition(L,resolution=1)
  nx.set_node_attributes(L, partition, 'modularity')
  listcommunities = set(partition.values())
  # print("Number of communities:", len(listcommunities))
  mod_assortivity = nx.attribute_assortativity_coefficient(L, attribute='modularity')
  # print(mod_assortivity)

"""## Export network"""

def export_network():
  print("export network")
  data1 = json_graph.node_link_data(L)
  networkname = congregation + '/' + congregation + "_network.json"
  f = open(networkname, "w")

  s1 = json.dump(data1, f, indent=2, sort_keys=True, default=str)

  # print(s1)

  # networkname = congregation + '/' + congregation + "_network.json"

  # f = open(networkname, "w")
  # f.write(s1)
  f.close()

"""## Export network statistics"""

def sample_path_lengths(G, nodes=None, trials=1000):
    if nodes is None:
        nodes = list(G)
    else:
        nodes = list(nodes)

    pairs = np.random.choice(nodes, (trials, 2))
    lengths = [nx.shortest_path_length(G, *pair)
               for pair in pairs]
    return lengths

def estimate_path_length(G, nodes=None, trials=1000):
    return np.mean(sample_path_lengths(G, nodes, trials))

def gender_mix():
  print("get gender mixing")
  global mix
  global mixmat
  global mapping
  # measure of segregation
  mapping = {'male': 0, 'female': 1, 'unknown': 2}
  mixmat = nx.attribute_mixing_matrix(L, 'gender', mapping=mapping)
  mixmat[mapping['male'], mapping['female']]
  # count of attribute pairs
  mix = nx.attribute_mixing_dict(L, "gender")
  # print("female-female", mix["female"]["female"])
  # print("male-female", mix["male"]["female"])
  # print("male-male", mix["male"]["male"])

# int((nodegender.gender == 'female').sum())

"""### Summary statistics"""

def get_degrees():
  print("get degree sequence")
  global average_degree
  degrees = dict(L.degree())
  sum_of_edges = sum(degrees.values())
  average_degree = sum_of_edges/L.number_of_nodes()

def make_stats():
  print("make stats")
  global mapping
  start = df['year'].min()
  end = df['year'].max()
  yrs = end - start
  av_nodes = int(L.number_of_nodes()/yrs)
  av_edges = int(L.number_of_edges()/yrs)
  total_bridges = female_bridges + male_bridges
  if total_bridges > 0:
    bridge_proportion = female_bridges/total_bridges
  else: 
    bridge_proportion = 0
  statsdict = {
      "congregation": congregation,
      "start": start,
      "end": end,
      "duration": int(end - start),
      "total nodes": G.number_of_nodes(),
      "total edges": G.number_of_edges(),
      "connected nodes": L.number_of_nodes(),
      "connected edges": L.number_of_edges(),
      "av_nodes": av_nodes,
      "av_edges": av_edges,
      "components": len(connected_subgraphs),
      "gender assortativity": gender_assortivity,
      "community assortativity": mod_assortivity,
      "degree assortativity": nx.degree_assortativity_coefficient(L),
      "community count": len(listcommunities),
      "average density": nx.density(L),
      "average degree": average_degree,
      "path length": estimate_path_length(L),
      "clustering": average_clustering(L),
      "female": int(len([x for x,y in L.nodes(data=True) if y['gender']=="female"])),
      "male": int(len([x for x,y in L.nodes(data=True) if y['gender']=="male"])),
      "unknown": int(len([x for x,y in L.nodes(data=True) if y['gender']=="unknown"])),
      "female_bridges": female_bridges,
      "male_bridges": male_bridges,
      "pct_bridges": (male_bridges + female_bridges) / L.number_of_nodes(),
      "gender_node_proportion": int(len([x for x,y in L.nodes(data=True) if y['gender']=="female"]))/L.number_of_nodes(),
      "bridge_proportion": bridge_proportion,
      "male-female": mixmat[mapping['male'], mapping['female']],
      "male-male": mixmat[mapping['male'], mapping['male']],
      "female-female": mixmat[mapping['female'], mapping['female']],
      "giant_percent_nodes": giant_percent_nodes,
      "giant_percent_edges": giant_percent_edges,
      "latitude": latitude,
      "longitude": longitude,
      "denomination": denomination
      # "triad_unconnected": len(triad_class[0]),
      # "triad_dyad": len(triad_class[1]),
      # "triad_incomplete": len(triad_class[2]),
      # "triad_complete": len(triad_class[3]),
  }
  # print(statsdict)
  print("make the dataframes")
  littlestatsdf = pd.DataFrame([statsdict])
  statsdfs.append(littlestatsdf)

  degseqdict = {
      "congregation": congregation,
      "degreeseq": degrees
  }
  littledegdf = pd.DataFrame([degseqdict])
  degdfs.append(littledegdf)
  # statsfile = congregation + '/' + congregation + "_network_stats.json"
  # with open(statsfile, "w") as outfile:
  #     json.dump(degseqdict, outfile, indent=2)

"""### Degree histogram"""

def export_histogram(): 
  print("export histogram") 
  histdf = pd.DataFrame(nx.degree_histogram(G))
  histdf.rename(columns={0: 'count'}, inplace=True)
  histdf.index.names = ['degree']
  histdf
  histfile = congregation + '/' + congregation + "_hist.csv"
  histdf.to_csv(histfile)

"""## Download everything"""

def download():
  shutil.make_archive(congregation, 'zip', congregation)
  # files.download(congregation + '.zip')

def cleanup():
  shutil.rmtree(congregation)
  os.remove(congregation + '.zip')

for c in remote_files:
  print(c['congregation'])
  setup(c['congregation'], c['filename'], c['denomination'], c['latitude'], c['longitude'])
  getedges()
  makegraph()
  make_giant()
  get_degseq()
  # get_triads()
  get_nodes()

  assign_gender()
  gender_unknown()
  set_gender()
  gender_assort(L)

  assign_race()
  set_race()

  set_degree()
  set_end()

  get_communities()

  make_years()

  set_bridges(L)

  export_density()

  export_network()

  gender_mix()
  get_degrees()

  make_stats()
  export_histogram()

  # download()
  # cleanup()

statdf = pd.concat(statsdfs, ignore_index=True)
degdf = pd.concat(degdfs, ignore_index=True)
statdf.to_json("network_stats.json", orient='records', indent=2)
# degdf.to_json("degreeseq.json", orient='records', indent=2)

# for c in remote_files:
#   cleanup()